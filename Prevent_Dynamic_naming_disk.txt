Prevent affect of dynamic disk naming on osd of rook-ceph

I got below error log on osd.3 of pod 
kubectl logs -n rook-ceph rook-ceph-osd-3-65f4cfdf-4n72g -c activate
RuntimeError: Error: ['lsblk: /dev/sdf: not a block device']

also, there is no the /dev/sdf on vm node. It seems the disk is not delete but disk name changed to sda.
sda           8:0    0   28G  0 disk
sdb           8:16   0   30G  0 disk
├─sdb1        8:17   0  286M  0 part /boot/efi
├─sdb2        8:18   0    1G  0 part /boot
.
.
sdd           8:48   0   28G  0 disk
sde           8:64   0   28G  0 disk

to prevent affect same change dynamic disk name, also better control and management on all disk name, we should use persistent disk identifiers.

# needed below steps.

1- there are many way to change dynamic disk name based on id, uuid, partuuid, etc...
but we use raw disks for osd, so there are no id or uuid for them. I use ID_PATH.

udevadm info --query=all --name=/dev/sdc | grep ID_PATH=
E: ID_PATH=pci-0000:03:00.0-scsi-0:0:2:0
udevadm info --query=all --name=/dev/sdd | grep ID_PATH=
E: ID_PATH=pci-0000:03:00.0-scsi-0:0:3:0
udevadm info --query=all --name=/dev/sde | grep ID_PATH=
E: ID_PATH=pci-0000:03:00.0-scsi-0:0:4:0

vi /etc/udev/rules.d/99-rook-ceph.rules
# Persistent name for the first disk - sdc
SUBSYSTEM=="block", ENV{ID_PATH}=="pci-0000:03:00.0-scsi-0:0:2:0", SYMLINK+="rook-ceph-disk-11"

# Persistent name for the second disk - sdd
SUBSYSTEM=="block", ENV{ID_PATH}=="pci-0000:03:00.0-scsi-0:0:3:0", SYMLINK+="rook-ceph-disk-12"

# Persistent name for the third disk - sde
SUBSYSTEM=="block", ENV{ID_PATH}=="pci-0000:03:00.0-scsi-0:0:4:0", SYMLINK+="rook-ceph-disk-13"

udevadm control --reload-rules
udevadm trigger

ls -ltrh /dev/rook-ceph-disk-1*
lrwxrwxrwx 1 root root 3 Oct 28 07:29 /dev/rook-ceph-disk-11 -> sdc
lrwxrwxrwx 1 root root 3 Oct 28 07:29 /dev/rook-ceph-disk-12 -> sdd
lrwxrwxrwx 1 root root 3 Oct 28 07:29 /dev/rook-ceph-disk-13 -> sde


# 2- change ROOK_ENABLE_DISCOVERY_DAEMON to default value (false). to prevent auto discovery raw disks.
# vim ~/rook-1.14.2/deploy/examples/operator.yaml
#   ROOK_ENABLE_DISCOVERY_DAEMON: "false"

# kubectl apply -f operator.yaml

3- configure nodes and disks
vim cluster.yaml
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    config:
    nodes:
      - name: "master"
        devices: # specific devices to use for storage can be specified for each node
          - name: "/dev/rook-ceph-disk-01"
          - name: "/dev/rook-ceph-disk-02"
          - name: "/dev/rook-ceph-disk-03"
      - name: "worker1"
        devices: # specific devices to use for storage can be specified for each node
          - name: "/dev/rook-ceph-disk-11"
          - name: "/dev/rook-ceph-disk-12"
          - name: "/dev/rook-ceph-disk-13"
      - name: "worker2"
        devices: # specific devices to use for storage can be specified for each node
          - name: "/dev/rook-ceph-disk-21"
          - name: "/dev/rook-ceph-disk-22"
          - name: "/dev/rook-ceph-disk-23"

kubectl apply -f cluster.yaml

4- result
ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME               STATUS  REWEIGHT  PRI-AFF
-1         0.24568  root default
-3         0.08189      host master
 0    hdd  0.02730          osd.0               up   1.00000  1.00000
 1    hdd  0.02730          osd.1               up   1.00000  1.00000
 2    hdd  0.02730          osd.2               up   1.00000  1.00000
-7         0.08189      host worker1
 6    hdd  0.02730          osd.6               up   1.00000  1.00000
 7    hdd  0.02730          osd.7               up   1.00000  1.00000
 8    hdd  0.02730          osd.8               up   1.00000  1.00000
-5         0.08189      host worker2
 3    hdd  0.02730          osd.3               up   1.00000  1.00000
 4    hdd  0.02730          osd.4               up   1.00000  1.00000
 5    hdd  0.02730          osd.5               up   1.00000  1.00000
